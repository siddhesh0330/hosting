#1B:Convert the given Text to Speech

from gtts import gTTS
import os
f = open("C:\\Users\\siddh\\PycharmProjects\\NLP_PRACTICE\\test.txt")
x = f.read()
language = 'en'
audio = gTTS(text = x, lang = language)
audio.save("C:\\Users\\siddh\\PycharmProjects\\NLP_PRACTICE\\speech.wav")
os.system("C:\\Users\\siddh\\PycharmProjects\\NLP_PRACTICE\\speech.wav")


#1C: Convert audio file to Text (Speech to Text).
import speech_recognition as sr
filename = "C:\\Users\\siddh\\PycharmProjects\\NLP_PRACTICE\\2.wav"
r = sr.Recognizer()
with sr.WavFile(filename) as source:
    audio = r.record(source)
text = r.recognize_google(audio)
print(text)
print("Speech to Text converted successfully")


"""
02
Study of various Corpus – Brown, Inaugural, Reuters, udhr with various
methods like fields, raw, words, sents, categories.
import spacy as spacy
"""

#Brown Corpus
import nltk
nltk.download('brown')
from nltk.corpus import brown
print(brown.categories())
print(brown.words(categories='news'))
print(brown.words(fileids=['cg22']))
print(brown.sents(categories=['news', 'editorial', 'reviews']))
news_text = brown.words(categories='news')
fdist = nltk.FreqDist(w.lower() for w in news_text)
modals = ['can', 'could', 'may', 'might', 'must', 'will']
for m in modals:
    print(m + ':', fdist[m], end=' ')




#Inaugural Corpus
import nltk
nltk.download('inaugural')
from nltk.corpus import inaugural
inaugural.fileids()
[fileid[:4] for fileid in inaugural.fileids()]
cfd = nltk.ConditionalFreqDist(
    (target, fileid[:4])
    for fileid in inaugural.fileids()
    for w in inaugural.words(fileid)
    for target in ['america', 'citizen']
    if w.lower().startswith(target))
cfd.plot()



#Reuters Corpus
import nltk
nltk.download('reuters')
from nltk.corpus import reuters
print(reuters.categories())
print(reuters.categories('training/9865'))
print(reuters.categories(['training/9865', 'training/9880']))
print(reuters.fileids('barley'))
print(reuters.fileids(['barley', 'corn']))
print(reuters.words('training/9865')[:14])
print(reuters.words(['training/9865', 'training/9880']))
print(reuters.words(categories=['barley', 'corn']))



#Udhr Corpus
import nltk
nltk.download('udhr')
from nltk.corpus import udhr
print(udhr.fileids())
print(udhr.words('Javanese-Latin1')[11:])
languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar',
'Ibibio_Efik']
cfd = nltk.ConditionalFreqDist(
    (lang, len(word))
    for lang in languages
    for word in udhr.words(lang + '-Latin1'))
cfd.plot(cumulative=True)



# E) Common Methods
import spacy
nlp=spacy.load("en_core_web_sm")
text = 'i live in Goregoan.'
for token in nlp(text):
    print(token.text, '->', token.dep_, '->', token.head.text)


import spacy

nlp=spacy.load("en_core_web_sm")
text='ram is a Intelligent boy.'
for token in nlp(text):
    print(token.text,'->',token.pos_,'->',token.tag_)




# 2B
import os, os.path
from nltk.corpus.reader import WordListCorpusReader

path = os.path.expanduser('~/natural_language_toolkit_data')
reader_corpus = WordListCorpusReader('.', ['wordsfile.txt'])
w = reader_corpus.words()
print(w)




# 2C
# Study Conditional frequency distributions

import nltk
nltk.download('brown')
from nltk.corpus import brown
genre_word = [
(genre, word)
for genre in ['news', 'romance']
for word in brown.words(categories=genre)
]
print('Length of the Genre word is ', len(genre_word))
print(genre_word[:4])
print(genre_word[-4:])



# C2) Study of tagged corpora with methods like tagged_sents, tagged_words.
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('nps_chat')
nltk.download('treebank')
nltk.download('conll2000')
from nltk.tokenize import word_tokenize
text=word_tokenize("And now for something completely different")
nltk.pos_tag(text)
print("\nBrown corpus: ", nltk.corpus.brown.tagged_words())
print("\nUniversal tagset: ", nltk.corpus.brown.tagged_words(tagset='universal'))
print("\nNPS chat corpus: ", nltk.corpus.nps_chat.tagged_words())
print("\nCONLL2000 corpus: ", nltk.corpus.conll2000.tagged_words())
print("\nTreebank corpus: ", nltk.corpus.treebank.tagged_words())



#C1 updated : Study Conditional frequency distributions

import nltk
from nltk.corpus import brown

# Download the Brown Corpus if not already downloaded
nltk.download('brown')

# Load the Brown Corpus
brown_corpus = brown.words(categories=['news', 'romance'])

# Create a Conditional Frequency Distribution
cfd = nltk.ConditionalFreqDist((genre, word.lower())
                                for genre in ['news', 'romance']
                                for word in brown.words(categories=genre))

# Print the most common words in each genre
print("Most common words in 'news':", cfd['news'].most_common(10))
print("Most common words in 'romance':", cfd['romance'].most_common(10))


#C2 updated : Study of tagged corpora with methods like tagged_sents, tagged_words.
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('nps_chat')
nltk.download('treebank')
nltk.download('conll2000')

text = word_tokenize("And now for something completely different")

tagged_text = nltk.pos_tag(text)

print("Tagged Text:")
print(tagged_text)

brown_tagged_sents = nltk.corpus.brown.tagged_sents()
print("\nTagged Sentences from Brown Corpus:")
for i, tagged_sent in enumerate(brown_tagged_sents[:3], start=1):
    print(f"Sentence {i}: {tagged_sent}")
treebank_tagged_words = nltk.corpus.treebank.tagged_words()
print("\nTagged Words from Treebank Corpus:")
for i, (word, tag) in enumerate(treebank_tagged_words[:10], start=1):
    print(f"Word {i}: {word} => Tag: {tag}")




# 2D : Write a program to find the most frequent noun tags.
import nltk
from nltk.corpus import brown
noundist = nltk.FreqDist(
    w2 for ((w1, t1), (w2, t2)) in nltk.bigrams(brown.tagged_words(tagset="universal"))
    if w1.lower() == "the" and t2 == "NOUN"
)
print(noundist.most_common(10))


#2E Map Words to Properties Using Python Dictionaries
build = {'CPU': 'AMD Ryzen 5 5600X', 'GPU': 'AMD RX 6500 XT'}
print(build)
# Declaring empty dict
parts = {}
parts[1] = 'CPU'
parts[2] = 'GPU'
parts[3] = 'RAM'
print(parts)


# 2F Study DefaultTagger, Regular expression tagger, UnigramTagger
# Default Tagger

import nltk
nltk.download('brown')
nltk.download('punkt')
from nltk.corpus import brown
brown_tagged_sents = brown.tagged_sents(categories='news')
tags = [tag for (word, tag) in brown.tagged_words(categories='news')]
print("Most used tag: ", nltk.FreqDist(tags).max())
# Creating a tagger that tags everything as NN
raw = 'I do not like green eggs and ham, I do not like them Sam I am!'
tokens = nltk.word_tokenize(raw)
default_tagger = nltk.DefaultTagger('NN')
print("Tagged with NN: \n", default_tagger.tag(tokens))


# F2) Regular expression tagger

from nltk.corpus import brown
from nltk import word_tokenize
from nltk import RegexpTagger
patterns = [
(r'.*ed$', 'VBD'), # simple past
(r'.*es$', 'VBZ'), # 3rd singular present
(r'.*ould$', 'MD'), # modals
(r'.*\'s$', 'NN$'), # possessive nouns
(r'.*s$', 'NNS'), # plural nouns
(r'^-?[0-9]+(\.[0-9]+)?$', 'CD'), # cardinal numbers
(r'.*', 'NN') # nouns (default)
]
regexp_tagger = RegexpTagger(patterns)
brown_sents = brown.sents(categories='news')
print('Tagged with RegexpTagger:')
print(regexp_tagger.tag(brown_sents[3]))


# F3) Unigram tagger
import nltk
from nltk.corpus import brown
brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
print('Sentence 2007 from brown corpus: \n', brown_sents[2007])
print('\nTagged words from sentence 2007:',unigram_tagger.tag(brown_sents[2007]))
print('\nEvaluation of UnigramTagger:',unigram_tagger.evaluate(brown_tagged_sents))


# 3A : A. Study of Wordnet Dictionary with methods as
# synsets, definitions, examples, antonyms.

import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet

def _three_a():
    definition_list = list()
    example_list = list()
    syns = wordnet.synsets('Morning')
    print(f'\n{syns}\n')
    print(f'\nsingle element of synset\n{syns[0].name()}\n')
    print(f'\njust the word\n{syns[0].lemmas()[0].name()}\n')
    print(f'\ndefinition of single element\n{syns[0].definition()}\n')
    print(f'\nExamples of single synonym\n{syns[0].examples()}\n')
    for defined_word in syns:
        definition_list.append({
        defined_word.name(): defined_word.definition()
    })
    for example in syns: example_list.append({
        example.name(): example.examples()}
    )
    print(f'\ndefinition_list\n{definition_list}\n')
    print(f'\nexample_list\n{example_list}\n')
_three_a()


# 3B : Study lemmas, hyponyms, hypernyms, entailments
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
def _three_b():
    lemma_names = wordnet.synset('car.n.01').lemma_names()
    print(f'\nlemma_names\n{lemma_names}\n')
    hyponyms_list = list()
    printer_hyponyms = wordnet.synset('printer.n.03')
    for synset in printer_hyponyms.hyponyms():
        for lemma in synset.lemmas():
            hyponyms_list.append(lemma.name())
            print(f'\nhyponyms of word printer\n{sorted(hyponyms_list)}')
            hypernyms_list = list()
            for synset in printer_hyponyms.hypernyms():
                for lemma in synset.lemmas():
                    hypernyms_list.append(lemma.name())
                print(f'\nhypernyms of word printer\n{sorted(hypernyms_list)}')
        entailed_word = wordnet.synset('eat.v.01').entailments()
    print(f'\nentailment\n{entailed_word}\n')
_three_b()


#3C : Write a program using python to find synonym and antonym of word "active" using Wordnet
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
def _three_c():
    synonyms = []
    antonyms = []
    for syn in wordnet.synsets("active"):
        for l in syn.lemmas():
            synonyms.append(l.name())
            if l.antonyms():
                antonyms.append(l.antonyms()[0].name())
        print(f'\nsynonyms\n{set(synonyms)}')
    print(f'\nantonyms\n{set(antonyms)}')
_three_c()


#3D : Compare two nouns
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
def _three_d(n1, n2):
    w1 = wordnet.synset(n1)
    w2 = wordnet.synset(n2)
    print(f'\nsimilarity between {n1} and {n2}')
    print('{0:.2f}%\n'.format(w1.wup_similarity(w2)*100))
_three_d('puma.n.01', 'cat.n.01')
_three_d('bike.n.01', 'wheel.n.01')
_three_d('fruit.n.01', 'flower.n.01')



#3E : Handling stopword. Using nltk Adding or Removing Stop Words in NLTK's Default Stop Word List Using Gensim Adding and Removing Stop Words in Default
# Gensim Stop Words List Using Spacy Adding and
# Removing Stop Words in Default Spacy Stop
# Words List

import nltk
import spacy
from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords
from nltk import word_tokenize
from spacy.lang.en import STOP_WORDS

nltk.download('stopwords')
nltk.download('punkt')
example_sent = "This is a sample sentence, showing off the stop words filtration. stopWord1"
def _there_e():
    example_sent = "This is a sample sentence, showing off the stop words filtration. stopWord1"
    def with_nltk():
        stopwords = nltk.corpus.stopwords.words('english')
        print(f'\ndefault stopwords\n{stopwords}\n')
        stopwords.append('newWord')
        print(f'\nstopwords after adding single word\n{stopwords}\n')
        stopwords.remove('newWord')
        print(f'\nstopwords after removing single word\n{stopwords}\n')
        newStopWords = ['stopWord1','stopWord2']
        stopwords.extend(newStopWords)
        print(f'\nstopwords after adding multiple words at once\n{stopwords}\n')
        word_tokens = word_tokenize(example_sent)
        filtered_sentence = [w for w in word_tokens if not w in STOPWORDS]
        filtered_sentence = []
        for w in word_tokens:
            if w not in STOPWORDS:
                filtered_sentence.append(w)
                print(f'\nword tokens{word_tokens}\n')
                print(f'\nsentence without stop words\n{filtered_sentence}\n')
def with_gensim():
    print(f'\nDEFAULT gensim stop_words{STOPWORDS}')
my_stop_words = STOPWORDS.union(set(['mystopword1', 'stopWord1']))
print(f'\nupdated_stop_word_list\n{my_stop_words}\n')
my_stop_words = set(my_stop_words).remove('a')
filtered_sentence = remove_stopwords(example_sent)
print(f'\nGensim - remove stopwords\n{filtered_sentence}\n')

def with_spacy():
    spacy_model = spacy.load('en_core_web_sm')
    print(f'\nDEFAULT SPACY stop_words{STOP_WORDS}')
STOP_WORDS.add("Test")
print(f'\nupdated stop word list spacy\n{STOP_WORDS}\n')
STOP_WORDS.remove("Test")
print(f'\nstop word list after remove -- spacy\n{STOP_WORDS}\n')
with_gensim()
with_spacy()
_there_e()


#4a : tokenization split function
import re
row = 'hi i am body'
print(row.split())

#4b tokenization regex
row = 'hi i am body'
print(re.findall("[\w']+", row))

#4c tokenization NLTK
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
row = 'hi i am body'
print(word_tokenize((row)))

#4D tokenization spacy
from spacy.lang.en import English
modal = English()
row = 'hi i am body'
token = []
for i in modal(row):
    token.append(i)
print(token)

#4e tokenization keras colab only
from keras.preprocessing.text import text_to_word_sequence
row = 'hi i am body'
print(text_to_word_sequence(row))

#4f tokenization gensim colab only
from gensim.utils import tokenize
row = 'hi i am body'
print(tokenize(row))


#  5a  NRE Tagging and chunking of user defined text.
from nltk import pos_tag,RegexpParser
import nltk
row = 'i am a boy'
patttan = """ mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC.?>}"""
nltk.download('averaged_perceptron_tagger')
token = row.split()
print(token)
tag = pos_tag((token))
print(tag)
chunker = RegexpParser(patttan)
output = chunker.parse(tag)
print(output)

# 5b NER recognition of user defined text spacy.
# NOTE : " python -m spacy download en_core_web_sm "
import spacy
row = "tom and sam playing for going to India"
nlp = spacy.load("en_core_web_sm")
ex = nlp(row)
print([(X.text, X.label_) for X in ex.ents])

# 5c NER recognition with diagram using NLTK corpus – Treebank
import nltk
from nltk import word_tokenize,pos_tag,ne_chunk
row = 'Peterson first suggested the name "opensource" at Palo Alto, California'
nltk.download('maxent_ne_chunker')
nltk.download('words')
word = word_tokenize((row))
tag = pos_tag(word)
chunks = ne_chunk(tag)
print(chunks)
for ne in chunks:
    if hasattr(ne, "label"):
        print(ne.label(), ne[0:])
chunks.pretty_print()

#6a : grammer using nltk. Analyze a sentence using the same
import nltk
grammar = nltk.CFG.fromstring("""
S -> NP VP
VP -> V NP | V NP PP
PP -> P NP
V -> "saw"|"eat"
NP -> "john"|"shone"| Det N | Det N PP
Det -> "a"|"an"|"the"|"my"
N -> "man"|"dog"|"home"
P -> "in" | "on" | "by" | "with" """)
row = "john saw shone"
words = row.split()
print(words)
modal = nltk.RecursiveDescentParser(grammar)
print(list(modal.parse(words)))
for tree in modal.parse(words):
    print(tree)
    tree.pretty_print()

# 6B:  Accept the input string with Regular expression of FA: 101+ .
import re
def FA(s):
    pattran = '^101'
    if re.match(pattran,s):
        print("accept")
    else:
        print("reject")
inputs = ['1', '10101', '101', '10111', '101101',""]
for i in inputs:
    (FA(i))

# 6c :  Accept the input string with Regular expression of FA: (a+b)*bba
def FA(s):
    pattern = r'^(a+b)*bba$'
    if re.match(pattern, s):
        print(f"{s}: accept")
    else:
        print(f"{s}: reject")
inputs = ['bba', 'ababbba', 'abba','abb', 'baba','bbb','aba']
for i in inputs:
    (FA(i))

# 6d : shift reduce, Implementation of Deductive Chart Parsing using context free grammar and a given sentence.
print( '--------- 6d shift reduce ------\n')
print("Shift reduce")
sr_parse = nltk.ShiftReduceParser(grammar, trace=2)
sent = "saw a dog".split()
for tree1 in sr_parse.parse(sent):
    print(tree1)
    tree1.pretty_print()


#7 : Study PorterStemmer, LancasterStemmer, RegexpStemmer,SnowballStemmer Study WordNetLemmatizer
# 7a Study PorterStemmer
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
poter = PorterStemmer()
words = ["gene", "genes", "genesis", "genetic", "generic", "general"]
for i in words:
    print(poter.stem(i))
sentance = ("Heya Siddhesh, do you know it is important to be pythonly  while pythoning with python language. Stay being a pythoner")
tokan = word_tokenize(sentance)
for i in tokan:
    print(poter.stem(i))

# 7b  Study LancasterStemmer
import nltk
from nltk.tokenize import word_tokenize
from  nltk.stem import LancasterStemmer
modal = LancasterStemmer()
terms = ["enjoy", "enjoying", "enjoyed", "enjoyable", "enjoyment", "enjoyful"]
# check for word
for i in terms:
    print(modal.stem(i))
# check for sentence
sentence = "Heya Harshal, Why is it so with the dancers that when dancers dance, they dance as if they are dancing in the air?"
token = word_tokenize(sentance)
for i in token:
    print(modal.stem(i))
# check for file
file = open('row.txt')
list_of_line = file.readline()
token = word_tokenize(list_of_line[0])
for i in token :
    print(modal.stem(i))

# 7c Study SnowballStemmer
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import  SnowballStemmer
modal1 = SnowballStemmer('english')
modal2 = SnowballStemmer('dutch')
words1 = ["reeba", "cheerful", "bravery","drawing", "satisfactorily", "publisher", "painful",
"hardworking", "keys"]
words2 = ["reeba", "bessen", "vriendelijkheid", "hobbelig"]
for i in words1:
    print(modal1.stem(i))
for i in words2:
    print(modal2.stem(i))

#  7d  Study RegexpStemmer
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from nltk.stem import RegexpStemmer
regexp = RegexpStemmer('ing$|s$|e$|able$|ment$|less$|ly$|ion$', min=4)
print(regexp.stem('cars'))
print(regexp.stem('bee'))
print(regexp.stem('compute'))#
terms = ["reebas", "stemming", "mentally", "ease","rockstar", "frictionless",
"management","flowers", "advisable", "friction"]
print("\n2. Performing regexp stemming on a list of words")
for each_term in terms:
    print(regexp.stem(each_term))


# 7e WordNetLemmatizer
import nltk
from nltk.tokenize import word_tokenize
from  nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
modal_e = WordNetLemmatizer()
sentence = "Heyaa Ishank, how are you doing? Keep digging in for the sentences to observe lemmatization!"
words = word_tokenize(sentance)
for i in words:
    print("".join(modal_e.lemmatize(i)))


#8 : Implement Naive Bayes classifier
import sklearn
from sklearn.metrics  import   classification_report,confusion_matrix,accuracy_score
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import  train_test_split
from sklearn.naive_bayes import GaussianNB
data = load_breast_cancer()
x = data.data
y = data.target
x_tarin,x_test,y_train,y_test = train_test_split(x,y,test_size=0.6,random_state=0)
modal = GaussianNB()
modal.fit(x_tarin,y_train)
yp = modal.predict(x_test)
print("confusion_matrix" , confusion_matrix(y_test,yp))
print("classification report" , classification_report(y_test,yp))
print("accuracy" , accuracy_score(y_test,yp)*100)


3
